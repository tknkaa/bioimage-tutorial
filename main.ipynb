{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7436152,"sourceType":"datasetVersion","datasetId":4327785}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"BSD 3-Clause License\n\nCopyright (c) 2017-2022, Pytorch contributors\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:27.154149Z","iopub.execute_input":"2025-06-08T07:17:27.154786Z","iopub.status.idle":"2025-06-08T07:17:27.732241Z","shell.execute_reply.started":"2025-06-08T07:17:27.154763Z","shell.execute_reply":"2025-06-08T07:17:27.731289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/train'\nVAL_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/valid'\nTEST_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/test'\n\nTRAIN_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/train/_annotations.coco.json'\nVAL_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/valid/_annotations.coco.json'\nTEST_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/test/_annotations.coco.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:29.018103Z","iopub.execute_input":"2025-06-08T07:17:29.018522Z","iopub.status.idle":"2025-06-08T07:17:29.022517Z","shell.execute_reply.started":"2025-06-08T07:17:29.0185Z","shell.execute_reply":"2025-06-08T07:17:29.021749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(TRAIN_PATH)[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:30.258766Z","iopub.execute_input":"2025-06-08T07:17:30.259031Z","iopub.status.idle":"2025-06-08T07:17:30.452923Z","shell.execute_reply.started":"2025-06-08T07:17:30.25901Z","shell.execute_reply":"2025-06-08T07:17:30.452347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_sample = Image.open(TRAIN_PATH + \"/\" + os.listdir(TRAIN_PATH)[0])\nplt.imshow(image_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:34.085153Z","iopub.execute_input":"2025-06-08T07:17:34.085483Z","iopub.status.idle":"2025-06-08T07:17:34.463664Z","shell.execute_reply.started":"2025-06-08T07:17:34.08546Z","shell.execute_reply":"2025-06-08T07:17:34.462847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(TRAIN_ANN, \"r\") as f:\n    raw_coco = f.read()\n    train_coco = json.loads(raw_coco)\n    print(train_coco.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:37.201594Z","iopub.execute_input":"2025-06-08T07:17:37.202401Z","iopub.status.idle":"2025-06-08T07:17:37.288415Z","shell.execute_reply.started":"2025-06-08T07:17:37.202356Z","shell.execute_reply":"2025-06-08T07:17:37.287683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_coco[\"images\"][100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:38.680792Z","iopub.execute_input":"2025-06-08T07:17:38.681397Z","iopub.status.idle":"2025-06-08T07:17:38.685148Z","shell.execute_reply.started":"2025-06-08T07:17:38.681374Z","shell.execute_reply":"2025-06-08T07:17:38.684523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_coco[\"annotations\"][100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:43.268194Z","iopub.execute_input":"2025-06-08T07:17:43.268455Z","iopub.status.idle":"2025-06-08T07:17:43.272632Z","shell.execute_reply.started":"2025-06-08T07:17:43.268436Z","shell.execute_reply":"2025-06-08T07:17:43.27191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def image_id_to_image_and_mask(coco, dir_name, image_id):\n    image_path = coco[\"images\"][image_id][\"file_name\"]\n    image = np.array(Image.open(dir_name + \"/\" + image_path))\n\n    bboxes = []\n    for ann in coco[\"annotations\"]:\n        if ann[\"image_id\"] == image_id:\n            bboxes.append(ann[\"bbox\"])\n    mask = np.zeros((coco[\"images\"][image_id][\"height\"], coco[\"images\"][image_id][\"width\"]))\n    for bbox in bboxes:\n        x_min, y_min, width, height = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n        mask[y_min:y_min + height, x_min:x_min + width] = 255 \n\n    return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:44.591374Z","iopub.execute_input":"2025-06-08T07:17:44.591615Z","iopub.status.idle":"2025-06-08T07:17:44.596961Z","shell.execute_reply.started":"2025-06-08T07:17:44.591598Z","shell.execute_reply":"2025-06-08T07:17:44.596144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, mask = image_id_to_image_and_mask(train_coco, TRAIN_PATH, 0)\nplt.subplot(1, 2, 1)\nplt.imshow(Image.fromarray(image))\nplt.title(\"image\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(Image.fromarray(mask))\nplt.title(\"mask\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:49.293574Z","iopub.execute_input":"2025-06-08T07:17:49.294301Z","iopub.status.idle":"2025-06-08T07:17:49.693749Z","shell.execute_reply.started":"2025-06-08T07:17:49.294276Z","shell.execute_reply":"2025-06-08T07:17:49.692965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\n\nimport torch\nimport torch.utils.data\n\nfrom torchvision import models, datasets, tv_tensors\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:17:53.007809Z","iopub.execute_input":"2025-06-08T07:17:53.008306Z","iopub.status.idle":"2025-06-08T07:18:00.38329Z","shell.execute_reply.started":"2025-06-08T07:17:53.008282Z","shell.execute_reply":"2025-06-08T07:18:00.382628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transforms = v2.Compose(\n    [\n        v2.ToImage(),\n        v2.RandomPhotometricDistort(p=1),\n        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=1),\n        v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\ndataset = datasets.CocoDetection(TRAIN_PATH, TRAIN_ANN, transforms=transforms)\ndataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:18:01.888757Z","iopub.execute_input":"2025-06-08T07:18:01.889165Z","iopub.status.idle":"2025-06-08T07:18:01.925925Z","shell.execute_reply.started":"2025-06-08T07:18:01.889142Z","shell.execute_reply":"2025-06-08T07:18:01.925219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:18:05.135067Z","iopub.execute_input":"2025-06-08T07:18:05.135725Z","iopub.status.idle":"2025-06-08T07:18:05.139245Z","shell.execute_reply.started":"2025-06-08T07:18:05.135699Z","shell.execute_reply":"2025-06-08T07:18:05.138601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=4,\n    # We need a custom collation function here, since the object detection\n    # models expect a sequence of images and target dictionaries. The default\n    # collation function tries to torch.stack() the individual elements,\n    # which fails in general for object detection, because the number of bounding\n    # boxes varies between the images of the same batch.\n    collate_fn=lambda batch: tuple(zip(*batch)),\n)\n\nmodel = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=\"DEFAULT\", weights_backbone=\"DEFAULT\").train()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ncount = 0\nfor imgs, targets in data_loader:\n    if count == 30:\n        break\n    else:\n        count += 1\n        \n    optimizer.zero_grad()\n    loss_dict = model(imgs, targets)\n    losses = sum(loss for loss in loss_dict.values())\n    losses.backward()\n    optimizer.step()\n\n    print(f\"{[img.shape for img in imgs] = }\")\n    print(f\"{[type(target) for target in targets] = }\")\n    for name, loss_val in loss_dict.items():\n        print(f\"{name:<20}{loss_val:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T07:51:00.881922Z","iopub.execute_input":"2025-06-08T07:51:00.882852Z","iopub.status.idle":"2025-06-08T08:30:17.501967Z","shell.execute_reply.started":"2025-06-08T07:51:00.882825Z","shell.execute_reply":"2025-06-08T08:30:17.500712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:30:25.16529Z","iopub.execute_input":"2025-06-08T08:30:25.165777Z","iopub.status.idle":"2025-06-08T08:30:25.173779Z","shell.execute_reply.started":"2025-06-08T08:30:25.165755Z","shell.execute_reply":"2025-06-08T08:30:25.173204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(TEST_ANN, \"r\") as f:\n    raw_test_ann = f.read()\n    test_coco = json.loads(raw_test_ann)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:30:26.781963Z","iopub.execute_input":"2025-06-08T08:30:26.783033Z","iopub.status.idle":"2025-06-08T08:30:26.792658Z","shell.execute_reply.started":"2025-06-08T08:30:26.782996Z","shell.execute_reply":"2025-06-08T08:30:26.791187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\nimage_tensor = image_tensor.unsqueeze(0)\n\nwith torch.no_grad():\n    preds = model(image_tensor)\npred_masks = preds[0][\"masks\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:36.358728Z","iopub.execute_input":"2025-06-08T08:55:36.358981Z","iopub.status.idle":"2025-06-08T08:55:39.598141Z","shell.execute_reply.started":"2025-06-08T08:55:36.358965Z","shell.execute_reply":"2025-06-08T08:55:39.597343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_mask = pred_masks[0, 0]\npred_mask = pred_mask.cpu().numpy()\npred_mask = (pred_mask > 0.5).astype(np.uint8)\n# pred_mask = pred_mask * 255","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:39.599378Z","iopub.execute_input":"2025-06-08T08:55:39.599649Z","iopub.status.idle":"2025-06-08T08:55:39.603884Z","shell.execute_reply.started":"2025-06-08T08:55:39.599626Z","shell.execute_reply":"2025-06-08T08:55:39.603227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(Image.fromarray(pred_mask))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:40.959126Z","iopub.execute_input":"2025-06-08T08:55:40.95945Z","iopub.status.idle":"2025-06-08T08:55:41.163282Z","shell.execute_reply.started":"2025-06-08T08:55:40.959427Z","shell.execute_reply":"2025-06-08T08:55:41.16258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, mask = image_id_to_image_and_mask(test_coco, TEST_PATH, 0)\nmask = Image.fromarray(mask)\npred_mask = Image.fromarray(pred_mask)\n\nplt.subplot(1, 3, 1)\nplt.imshow(image)\nplt.title(\"image\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(mask)\nplt.title(\"mask\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(pred_mask)\nplt.title(\"mask\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:45.003724Z","iopub.execute_input":"2025-06-08T08:55:45.003996Z","iopub.status.idle":"2025-06-08T08:55:45.488801Z","shell.execute_reply.started":"2025-06-08T08:55:45.003974Z","shell.execute_reply":"2025-06-08T08:55:45.488215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:49.080788Z","iopub.execute_input":"2025-06-08T08:55:49.081265Z","iopub.status.idle":"2025-06-08T08:55:49.085444Z","shell.execute_reply.started":"2025-06-08T08:55:49.081229Z","shell.execute_reply":"2025-06-08T08:55:49.084646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"contours, hierarchy = cv2.findContours(np.array(mask).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\ncv.drawContours(image, contours, -1, (255,0,0), 3)\nplt.imshow(image)\nplt.savefig(\"original.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:55:51.530573Z","iopub.execute_input":"2025-06-08T08:55:51.531138Z","iopub.status.idle":"2025-06-08T08:55:51.853839Z","shell.execute_reply.started":"2025-06-08T08:55:51.531118Z","shell.execute_reply":"2025-06-08T08:55:51.853097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"contours, hierarchy = cv2.findContours(np.array(pred_mask).astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\ncv.drawContours(image, contours, -1, (0,255,0), 3)\nplt.imshow(image)\nplt.savefig(\"pred.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:56:59.60626Z","iopub.execute_input":"2025-06-08T08:56:59.606568Z","iopub.status.idle":"2025-06-08T08:56:59.934748Z","shell.execute_reply.started":"2025-06-08T08:56:59.606542Z","shell.execute_reply":"2025-06-08T08:56:59.933949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}