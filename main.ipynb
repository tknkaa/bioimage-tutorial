{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7436152,"sourceType":"datasetVersion","datasetId":4327785}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport json\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:28.31283Z","iopub.execute_input":"2025-06-08T02:37:28.313334Z","iopub.status.idle":"2025-06-08T02:37:29.498217Z","shell.execute_reply.started":"2025-06-08T02:37:28.313312Z","shell.execute_reply":"2025-06-08T02:37:29.497601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pycocotools.coco import COCO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:21.660446Z","iopub.execute_input":"2025-06-08T02:37:21.660767Z","iopub.status.idle":"2025-06-08T02:37:21.67613Z","shell.execute_reply.started":"2025-06-08T02:37:21.660743Z","shell.execute_reply":"2025-06-08T02:37:21.675299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/train'\nVAL_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/valid'\nTEST_PATH = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/test'\n\nTRAIN_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/train/_annotations.coco.json'\nVAL_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/valid/_annotations.coco.json'\nTEST_ANN = '/kaggle/input/brain-tumor-image-dataset-semantic-segmentation/test/_annotations.coco.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:22.785193Z","iopub.execute_input":"2025-06-08T02:37:22.785709Z","iopub.status.idle":"2025-06-08T02:37:22.789904Z","shell.execute_reply.started":"2025-06-08T02:37:22.785682Z","shell.execute_reply":"2025-06-08T02:37:22.789101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.listdir(TRAIN_PATH)[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:32.883501Z","iopub.execute_input":"2025-06-08T02:37:32.88439Z","iopub.status.idle":"2025-06-08T02:37:32.959035Z","shell.execute_reply.started":"2025-06-08T02:37:32.884361Z","shell.execute_reply":"2025-06-08T02:37:32.958394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_sample = Image.open(TRAIN_PATH + \"/\" + os.listdir(TRAIN_PATH)[0])\nplt.imshow(image_sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:34.19724Z","iopub.execute_input":"2025-06-08T02:37:34.198061Z","iopub.status.idle":"2025-06-08T02:37:34.542468Z","shell.execute_reply.started":"2025-06-08T02:37:34.198018Z","shell.execute_reply":"2025-06-08T02:37:34.541684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(TRAIN_ANN, \"r\") as f:\n    raw_coco = f.read()\n    coco = json.loads(raw_coco)\n    print(coco.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:36.558305Z","iopub.execute_input":"2025-06-08T02:37:36.558666Z","iopub.status.idle":"2025-06-08T02:37:36.585807Z","shell.execute_reply.started":"2025-06-08T02:37:36.558626Z","shell.execute_reply":"2025-06-08T02:37:36.585225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(coco[\"images\"][100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:38.027816Z","iopub.execute_input":"2025-06-08T02:37:38.028408Z","iopub.status.idle":"2025-06-08T02:37:38.032776Z","shell.execute_reply.started":"2025-06-08T02:37:38.028378Z","shell.execute_reply":"2025-06-08T02:37:38.031797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(coco[\"annotations\"][100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:39.286321Z","iopub.execute_input":"2025-06-08T02:37:39.286674Z","iopub.status.idle":"2025-06-08T02:37:39.291015Z","shell.execute_reply.started":"2025-06-08T02:37:39.286653Z","shell.execute_reply":"2025-06-08T02:37:39.290125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def image_id_to_image_and_mask(image_id):\n    image_path = coco[\"images\"][image_id][\"file_name\"]\n    image = np.array(Image.open(TRAIN_PATH + \"/\" + image_path))\n\n    bboxes = []\n    for ann in coco[\"annotations\"]:\n        if ann[\"image_id\"] == image_id:\n            bboxes.append(ann[\"bbox\"])\n    mask = np.zeros((coco[\"images\"][image_id][\"height\"], coco[\"images\"][image_id][\"width\"]))\n    for bbox in bboxes:\n        x_min, y_min, width, height = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n        mask[y_min:y_min + height, x_min:x_min + width] = 255 \n\n    return image, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:40.750836Z","iopub.execute_input":"2025-06-08T02:37:40.751119Z","iopub.status.idle":"2025-06-08T02:37:40.756653Z","shell.execute_reply.started":"2025-06-08T02:37:40.751097Z","shell.execute_reply":"2025-06-08T02:37:40.75588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image, mask = image_id_to_image_and_mask(0)\nplt.subplot(1, 2, 1)\nplt.imshow(Image.fromarray(image))\nplt.title(\"image\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(Image.fromarray(mask))\nplt.title(\"mask\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:42.410723Z","iopub.execute_input":"2025-06-08T02:37:42.411458Z","iopub.status.idle":"2025-06-08T02:37:42.807448Z","shell.execute_reply.started":"2025-06-08T02:37:42.411428Z","shell.execute_reply":"2025-06-08T02:37:42.80679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pathlib\n\nimport torch\nimport torch.utils.data\n\nfrom torchvision import models, datasets, tv_tensors\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:44.69693Z","iopub.execute_input":"2025-06-08T02:37:44.697196Z","iopub.status.idle":"2025-06-08T02:37:50.961748Z","shell.execute_reply.started":"2025-06-08T02:37:44.697179Z","shell.execute_reply":"2025-06-08T02:37:50.960958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transforms = v2.Compose(\n    [\n        v2.ToImage(),\n        v2.RandomPhotometricDistort(p=1),\n        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}),\n        v2.RandomIoUCrop(),\n        v2.RandomHorizontalFlip(p=1),\n        v2.SanitizeBoundingBoxes(),\n        v2.ToDtype(torch.float32, scale=True),\n    ]\n)\n\ndataset = datasets.CocoDetection(TRAIN_PATH, TRAIN_ANN, transforms=transforms)\ndataset = datasets.wrap_dataset_for_transforms_v2(dataset, target_keys=[\"boxes\", \"labels\", \"masks\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:55.103476Z","iopub.execute_input":"2025-06-08T02:37:55.103937Z","iopub.status.idle":"2025-06-08T02:37:55.123548Z","shell.execute_reply.started":"2025-06-08T02:37:55.103912Z","shell.execute_reply":"2025-06-08T02:37:55.122795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_loader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=4,\n    # We need a custom collation function here, since the object detection\n    # models expect a sequence of images and target dictionaries. The default\n    # collation function tries to torch.stack() the individual elements,\n    # which fails in general for object detection, because the number of bounding\n    # boxes varies between the images of the same batch.\n    collate_fn=lambda batch: tuple(zip(*batch)),\n)\n\nmodel = models.get_model(\"maskrcnn_resnet50_fpn_v2\", weights=None, weights_backbone=None).train()\n\nfor imgs, targets in data_loader:\n    loss_dict = model(imgs, targets)\n    # Put your training logic here\n\n    print(f\"{[img.shape for img in imgs] = }\")\n    print(f\"{[type(target) for target in targets] = }\")\n    for name, loss_val in loss_dict.items():\n        print(f\"{name:<20}{loss_val:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:37:57.316233Z","iopub.execute_input":"2025-06-08T02:37:57.317016Z","iopub.status.idle":"2025-06-08T02:39:12.299874Z","shell.execute_reply.started":"2025-06-08T02:37:57.31699Z","shell.execute_reply":"2025-06-08T02:39:12.298614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model, \"/kaggle/working/weights.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T02:40:39.61541Z","iopub.execute_input":"2025-06-08T02:40:39.616076Z","iopub.status.idle":"2025-06-08T02:40:39.855386Z","shell.execute_reply.started":"2025-06-08T02:40:39.616046Z","shell.execute_reply":"2025-06-08T02:40:39.854506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}